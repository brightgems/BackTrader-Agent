"""
Ollama æœåŠ¡éªŒè¯è„šæœ¬
æµ‹è¯• LLM Advisory ä¸æœ¬åœ° Ollama æœåŠ¡çš„é›†æˆ
"""

import os
import sys

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ° Python è·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from llm_advisory.llm_advisor import check_llm_service_availability
from llm_advisory.services.ollama_service import get_ollama_service


def test_ollama_connection():
    """æµ‹è¯• Ollama è¿æ¥"""
    print("=== Ollama è¿æ¥æµ‹è¯• ===")
    
    try:
        service = get_ollama_service()
        
        # æµ‹è¯•åŸºç¡€è¿æ¥
        if service.test_connection():
            print("âœ… Ollama æœåŠ¡è¿æ¥æˆåŠŸ")
        else:
            print("âŒ Ollama æœåŠ¡è¿æ¥å¤±è´¥")
            return False
        
        # è·å–å¯ç”¨æ¨¡å‹
        models = service.get_available_models()
        if models:
            print(f"âœ… å‘ç° {len(models)} ä¸ªæ¨¡å‹:")
            for model in models:
                print(f"   - {model}")
        else:
            print("âš ï¸  æœªå‘ç°æ¨¡å‹ï¼Œè¯·ä¸‹è½½æ¨¡å‹: ollama pull qwen3-vl")
            return False
        
        return True
        
    except Exception as e:
        print(f"âŒ è¿æ¥æµ‹è¯•å¤±è´¥: {e}")
        return False


def test_chat_completion():
    """æµ‹è¯•èŠå¤©å®ŒæˆåŠŸèƒ½"""
    print("\n=== èŠå¤©å®Œæˆæµ‹è¯• ===")
    
    try:
        service = get_ollama_service()
        
        messages = [
            {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªæµ‹è¯•åŠ©æ‰‹ï¼Œè¯·ç”¨ä¸­æ–‡å›ç­”ã€‚"},
            {"role": "user", "content": "è¯·ç®€å•ä»‹ç»ä¸€ä¸‹é‡åŒ–äº¤æ˜“"}
        ]
        
        response = service.create_chat_completion(
            messages=messages,
            model=os.getenv("OLLAMA_MODEL", default="qwen3-vl"),
            temperature=0.7,
            max_tokens=100
        )
        
        print("âœ… èŠå¤©å®Œæˆæµ‹è¯•æˆåŠŸ")
        print(f"å“åº”: {response['content'][:200]}...")
        return True
        
    except Exception as e:
        print(f"âŒ èŠå¤©å®Œæˆæµ‹è¯•å¤±è´¥: {e}")
        return False


def test_advisory_integration():
    """æµ‹è¯• LLM Advisory é›†æˆ"""
    print("\n=== LLM Advisory é›†æˆæµ‹è¯• ===")
    
    try:
        # æµ‹è¯•æœåŠ¡å¯ç”¨æ€§æ£€æŸ¥
        result = check_llm_service_availability("ollama")
        
        print(f"æœåŠ¡çŠ¶æ€: {'âœ… å¯ç”¨' if result['available'] else 'âŒ ä¸å¯ç”¨'}")
        print(f"æä¾›å•†: {result['provider']}")
        print(f"å¯ç”¨æä¾›å•†: {result.get('available_providers', [])}")
        
        if result['available']:
            print("âœ… LLM Advisory é›†æˆæµ‹è¯•æˆåŠŸ")
            return True
        else:
            print(f"âŒ LLM Advisory é›†æˆæµ‹è¯•å¤±è´¥: {result['details']}")
            return False
            
    except Exception as e:
        print(f"âŒ é›†æˆæµ‹è¯•å¤±è´¥: {e}")
        return False


def test_trading_prompt():
    """æµ‹è¯•äº¤æ˜“ç›¸å…³çš„æç¤ºè¯"""
    print("\n=== äº¤æ˜“æç¤ºè¯æµ‹è¯• ===")
    
    try:
        service = get_ollama_service()
        
        # æ¨¡æ‹Ÿäº¤æ˜“æ•°æ®
        trading_data = """
        è‚¡ç¥¨: AAPL
        å½“å‰ä»·æ ¼: 150.25
        ç§»åŠ¨å¹³å‡çº¿(10æ—¥): 148.50
        ç§»åŠ¨å¹³å‡çº¿(30æ—¥): 145.80
        RSI(14): 65
        è¶‹åŠ¿: ä¸Šæ¶¨
        """
        
        system_prompt = """ä½ æ˜¯ä¸€åä¸“ä¸šçš„é‡åŒ–äº¤æ˜“åˆ†æå¸ˆã€‚è¯·åŸºäºæä¾›çš„äº¤æ˜“æ•°æ®ç»™å‡ºåˆ†æå»ºè®®ã€‚
        è¯·ç”¨ä¸­æ–‡å›ç­”ï¼Œæ ¼å¼ä¸º:
        ä¿¡å·: [bullish/bearish/neutral/none]
        ä¿¡å¿ƒ: [0.0-1.0]
        åˆ†æ: [ç®€è¦åˆ†æ]"""
        
        user_prompt = f"è¯·åˆ†æä»¥ä¸‹äº¤æ˜“æ•°æ®:\n{trading_data}"
        
        response = service.generate_advisor_response(
            system_prompt=system_prompt,
            user_prompt=user_prompt,
            model="qwen3-vl"
        )
        
        print("âœ… äº¤æ˜“æç¤ºè¯æµ‹è¯•æˆåŠŸ")
        print(f"å“åº”é•¿åº¦: {len(response)} å­—ç¬¦")
        print(f"å“åº”é¢„è§ˆ: {response[:300]}...")
        return True
        
    except Exception as e:
        print(f"âŒ äº¤æ˜“æç¤ºè¯æµ‹è¯•å¤±è´¥: {e}")
        return False


def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("Ollama æœåŠ¡éªŒè¯è„šæœ¬")
    print("=" * 50)
    
    # æ£€æŸ¥ç¯å¢ƒå˜é‡
    print("ç¯å¢ƒå˜é‡æ£€æŸ¥:")
    ollama_url = os.getenv('OLLAMA_BASE_URL', 'æœªè®¾ç½®')
    ollama_model = os.getenv('OLLAMA_MODEL', 'æœªè®¾ç½®')
    print(f"OLLAMA_BASE_URL: {ollama_url}")
    print(f"OLLAMA_MODEL: {ollama_model}")
    
    # è¿è¡Œæµ‹è¯•
    tests = [
        test_ollama_connection,
        test_chat_completion,
        test_advisory_integration,
        test_trading_prompt
    ]
    
    results = []
    for test in tests:
        try:
            result = test()
            results.append(result)
        except Exception as e:
            print(f"æµ‹è¯•å¼‚å¸¸: {e}")
            results.append(False)
    
    # æ€»ç»“ç»“æœ
    print("\n" + "=" * 50)
    print("æµ‹è¯•æ€»ç»“:")
    passed = sum(results)
    total = len(results)
    
    print(f"é€šè¿‡: {passed}/{total} ä¸ªæµ‹è¯•")
    
    if passed == total:
        print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼Ollama æœåŠ¡é…ç½®æ­£ç¡®ã€‚")
        print("\nä¸‹ä¸€æ­¥:")
        print("1. è¿è¡Œ examples/ollama_advisory_example.py")
        print("2. é›†æˆåˆ°æ‚¨çš„äº¤æ˜“ç­–ç•¥ä¸­")
    else:
        print("âŒ éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥é…ç½®ã€‚")
        print("\nå¸¸è§é—®é¢˜:")
        print("1. ç¡®ä¿ Ollama å·²å®‰è£…å¹¶è¿è¡Œ")
        print("2. ä¸‹è½½æ¨¡å‹: ollama pull qwen3-vl")
        print("3. æ£€æŸ¥ .env æ–‡ä»¶é…ç½®")
        print("4. æŸ¥çœ‹è¯¦ç»†çš„é”™è¯¯ä¿¡æ¯")


if __name__ == "__main__":
    main()